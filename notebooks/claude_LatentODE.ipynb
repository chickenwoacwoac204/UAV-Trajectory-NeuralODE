{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c614f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604b3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f68cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (176, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "df = pd.read_csv(\"../data/processed/uav_hugging_face_dropped_20.csv\")\n",
    "print(f\"Original data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76a2285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "timestamps = df['timestamp'].values\n",
    "positions = df[['tx', 'ty', 'tz']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f0003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate time deltas between consecutive points\n",
    "time_deltas = np.diff(timestamps)\n",
    "time_deltas = np.insert(time_deltas, 0, 0)  # Add 0 for the first point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d753cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale coordinates to improve training\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "positions_scaled = scaler.fit_transform(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c987f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch tensors\n",
    "tensor_timestamps = torch.tensor(timestamps - timestamps[0], dtype=torch.float32).to(device)\n",
    "tensor_positions = torch.tensor(positions_scaled, dtype=torch.float32).to(device)\n",
    "tensor_time_deltas = torch.tensor(time_deltas, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e570ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ODE function base class (similar to what you had in test.py)\n",
    "class ODEF(nn.Module):\n",
    "    def forward_with_grad(self, z, t, grad_outputs):\n",
    "        \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\"\n",
    "        batch_size = z.shape[0]\n",
    "\n",
    "        out = self.forward(z, t)\n",
    "\n",
    "        a = grad_outputs\n",
    "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
    "            (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a),\n",
    "            allow_unused=True, retain_graph=True\n",
    "        )\n",
    "        # grad method automatically sums gradients for batch items, we have to expand them back \n",
    "        if adfdp and any(p is not None for p in adfdp):\n",
    "            adfdp = torch.cat([p_grad.flatten() if p_grad is not None else \n",
    "                            torch.zeros_like(p).flatten() \n",
    "                            for p_grad, p in zip(adfdp, self.parameters())]).unsqueeze(0)\n",
    "            adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
    "        else:\n",
    "            adfdp = torch.zeros(batch_size, 0).to(z)\n",
    "        if adfdt is not None:\n",
    "            adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
    "        else:\n",
    "            adfdt = torch.zeros(batch_size, 1).to(z)\n",
    "        return out, adfdz, adfdt, adfdp\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        p_shapes = []\n",
    "        flat_parameters = []\n",
    "        for p in self.parameters():\n",
    "            p_shapes.append(p.size())\n",
    "            flat_parameters.append(p.flatten())\n",
    "        return torch.cat(flat_parameters) if len(flat_parameters) > 0 else torch.tensor([]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0203606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_solve(z0, t0, t1, f):\n",
    "    \"\"\"\n",
    "    Simplest Euler ODE initial value solver\n",
    "    \"\"\"\n",
    "    h_max = 0.05\n",
    "    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n",
    "\n",
    "    h = (t1 - t0)/n_steps\n",
    "    t = t0\n",
    "    z = z0\n",
    "\n",
    "    for i_step in range(n_steps):\n",
    "        z = z + h * f(z, t)\n",
    "        t = t + h\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e5f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEAdjoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z0, t, flat_parameters, func):\n",
    "        assert isinstance(func, ODEF)\n",
    "        bs, *z_shape = z0.size()\n",
    "        time_len = t.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.zeros(time_len, bs, *z_shape).to(z0)\n",
    "            z[0] = z0\n",
    "            for i_t in range(time_len - 1):\n",
    "                z0 = ode_solve(z0, t[i_t], t[i_t+1], func)\n",
    "                z[i_t+1] = z0\n",
    "\n",
    "        ctx.func = func\n",
    "        ctx.save_for_backward(t, z.clone(), flat_parameters)\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dLdz):\n",
    "        \"\"\"\n",
    "        dLdz shape: time_len, batch_size, *z_shape\n",
    "        \"\"\"\n",
    "        func = ctx.func\n",
    "        t, z, flat_parameters = ctx.saved_tensors\n",
    "        time_len, bs, *z_shape = z.size()\n",
    "        # n_dim = np.prod(z_shape)\n",
    "        n_dim = 3\n",
    "        n_params = flat_parameters.size(0)\n",
    "\n",
    "        # Dynamics of augmented system to be calculated backwards in time\n",
    "        def augmented_dynamics(aug_z_i, t_i):\n",
    "            \"\"\"\n",
    "            tensors here are temporal slices\n",
    "            t_i - is tensor with size: bs, 1\n",
    "            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n",
    "            \"\"\"\n",
    "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  # ignore parameters and time\n",
    "\n",
    "            # Unflatten z and a\n",
    "            z_i = z_i.view(bs, *z_shape)\n",
    "            a = a.view(bs, *z_shape)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                t_i = t_i.detach().requires_grad_(True)\n",
    "                z_i = z_i.detach().requires_grad_(True)\n",
    "                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a)  # bs, *z_shape\n",
    "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
    "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
    "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n",
    "\n",
    "            # Flatten f and adfdz\n",
    "            func_eval = func_eval.view(bs, n_dim)\n",
    "            adfdz = adfdz.view(bs, n_dim) \n",
    "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
    "\n",
    "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
    "        with torch.no_grad():\n",
    "            ## Create placeholders for output gradients\n",
    "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
    "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
    "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
    "            # In contrast to z and p we need to return gradients for all times\n",
    "            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
    "\n",
    "            for i_t in range(time_len-1, 0, -1):\n",
    "                z_i = z[i_t]\n",
    "                t_i = t[i_t]\n",
    "                f_i = func(z_i, t_i).view(bs, n_dim)\n",
    "\n",
    "                # Compute direct gradients\n",
    "                dLdz_i = dLdz[i_t]\n",
    "                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "                # Adjusting adjoints with direct gradients\n",
    "                adj_z += dLdz_i\n",
    "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
    "\n",
    "                # Pack augmented variable\n",
    "                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1)\n",
    "\n",
    "                # Solve augmented system backwards\n",
    "                aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
    "\n",
    "                # Unpack solved backwards augmented system\n",
    "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
    "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
    "                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n",
    "\n",
    "                del aug_z, aug_ans\n",
    "\n",
    "            ## Adjust 0 time adjoint with direct gradients\n",
    "            # Compute direct gradients \n",
    "            dLdz_0 = dLdz[0]\n",
    "            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "            # Adjust adjoints\n",
    "            adj_z += dLdz_0\n",
    "            adj_t[0] = adj_t[0] - dLdt_0\n",
    "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac1b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        assert isinstance(func, ODEF)\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, z0, t=None, return_whole_sequence=False):\n",
    "        if t is None:\n",
    "            t = torch.tensor([0., 1.]).to(device)\n",
    "        t = t.to(z0.device)\n",
    "        z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func)\n",
    "        if return_whole_sequence:\n",
    "            return z\n",
    "        else:\n",
    "            return z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a0c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder for the Latent ODE model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # RNN to process sequential data\n",
    "        self.gru = nn.GRU(input_dim + 1, hidden_dim, batch_first=True)  # +1 for time delta\n",
    "        \n",
    "        # Output mean and log variance for the latent state\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x, time_delta):\n",
    "        # Combine position and time data\n",
    "        combined = torch.cat([x, time_delta.unsqueeze(-1)], dim=-1)\n",
    "        \n",
    "        # Process with GRU\n",
    "        _, h_n = self.gru(combined.unsqueeze(0))\n",
    "        h_n = h_n.squeeze(0)\n",
    "        \n",
    "        # Get latent parameters\n",
    "        mean = self.fc_mean(h_n)\n",
    "        logvar = self.fc_logvar(h_n)\n",
    "        \n",
    "        return mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5aa5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ODE function for the latent space dynamics\n",
    "class LatentODEFunc(ODEF):\n",
    "    def __init__(self, latent_dim, hidden_dim):\n",
    "        super(LatentODEFunc, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Neural network to approximate the derivative\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 1, hidden_dim),  # +1 for time\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, t):\n",
    "        # Expand t to match z's dimensions\n",
    "        t_expanded = t.expand(z.shape[0], 1)\n",
    "        \n",
    "        # Concatenate z and t\n",
    "        z_t = torch.cat([z, t_expanded], dim=1)\n",
    "        \n",
    "        # Compute derivative\n",
    "        return self.net(z_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b810b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decoder to map from latent space back to observation space\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "703aa3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Latent ODE model\n",
    "class LatentODE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim):\n",
    "        super(LatentODE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder, ODE function, and Decoder\n",
    "        self.encoder = Encoder(input_dim, latent_dim, hidden_dim).to(device)\n",
    "        self.ode_func = LatentODEFunc(latent_dim, hidden_dim).to(device)\n",
    "        self.decoder = Decoder(latent_dim, input_dim, hidden_dim).to(device)\n",
    "        \n",
    "        # Neural ODE solver\n",
    "        self.ode_solver = NeuralODE(self.ode_func).to(device)\n",
    "        \n",
    "    def encode(self, x, time_delta):\n",
    "        mean, logvar = self.encoder(x, time_delta)\n",
    "        \n",
    "        # Reparameterization trick for sampling\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z0 = mean + eps * std\n",
    "        \n",
    "        return z0, mean, logvar\n",
    "    \n",
    "    def forward(self, x, times, time_delta, return_latent=False):\n",
    "        # Encode to get initial latent state\n",
    "        z0, mean, logvar = self.encode(x, time_delta)\n",
    "        \n",
    "        # Solve ODE to get latent trajectory\n",
    "        z_trajectory = self.ode_solver(z0.unsqueeze(0), times, return_whole_sequence=True)\n",
    "        \n",
    "        # Decode latent trajectory to observations\n",
    "        x_reconstructed = self.decoder(z_trajectory)\n",
    "        \n",
    "        if return_latent:\n",
    "            return x_reconstructed, z_trajectory, mean, logvar\n",
    "        else:\n",
    "            return x_reconstructed, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6450ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create uniform time grid for reconstruction and evaluation\n",
    "min_time = tensor_timestamps[0].item()\n",
    "max_time = tensor_timestamps[-1].item()\n",
    "num_uniform_points = 300  # Adjust as needed for smoother interpolation\n",
    "uniform_times = torch.linspace(min_time, max_time, num_uniform_points).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d71e18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_dim = 3  # x, y, z coordinates\n",
    "latent_dim = 6  # Size of latent space (adjust as needed)\n",
    "hidden_dim = 32  # Size of hidden layers (adjust as needed)\n",
    "model = LatentODE(input_dim, latent_dim, hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e02541e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98df6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for VAE (reconstruction + KL divergence)\n",
    "def loss_function(x_reconstructed, x_target, mean, logvar):\n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = F.mse_loss(x_reconstructed, x_target)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Total loss (with weight on KL term)\n",
    "    return recon_loss + 0.01 * kl_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9033b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tracking loss\n",
    "train_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d82620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor of all available timestamps\n",
    "all_times = tensor_timestamps.unsqueeze(1)  # Shape [num_points, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "547aea8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Forward pass using all data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m x_reconstructed, mean, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_positions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_time_deltas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss, recon_loss, kl_loss \u001b[38;5;241m=\u001b[39m loss_function(\n\u001b[1;32m     14\u001b[0m     x_reconstructed\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# Remove batch dimension\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     tensor_positions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(x_reconstructed)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# Match dimensions\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     mean, \n\u001b[1;32m     17\u001b[0m     logvar\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/Pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[15], line 30\u001b[0m, in \u001b[0;36mLatentODE.forward\u001b[0;34m(self, x, times, time_delta, return_latent)\u001b[0m\n\u001b[1;32m     27\u001b[0m z0, mean, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x, time_delta)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Solve ODE to get latent trajectory\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m z_trajectory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mode_solver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_whole_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Decode latent trajectory to observations\u001b[39;00m\n\u001b[1;32m     33\u001b[0m x_reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z_trajectory)\n",
      "File \u001b[0;32m~/Pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mNeuralODE.forward\u001b[0;34m(self, z0, t, return_whole_sequence)\u001b[0m\n\u001b[1;32m      9\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1.\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mto(z0\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 11\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mODEAdjoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_whole_sequence:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m~/Pytorch/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mODEAdjoint.forward\u001b[0;34m(ctx, z0, t, flat_parameters, func)\u001b[0m\n\u001b[1;32m     10\u001b[0m     z[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m z0\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i_t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(time_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m         z0 \u001b[38;5;241m=\u001b[39m \u001b[43mode_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_t\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         z[i_t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m z0\n\u001b[1;32m     15\u001b[0m ctx\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;241m=\u001b[39m func\n",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m, in \u001b[0;36mode_solve\u001b[0;34m(z0, t0, t1, f)\u001b[0m\n\u001b[1;32m     10\u001b[0m z \u001b[38;5;241m=\u001b[39m z0\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[0;32m---> 13\u001b[0m     z \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m h \u001b[38;5;241m*\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     t \u001b[38;5;241m=\u001b[39m t \u001b[38;5;241m+\u001b[39m h\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m~/Pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m, in \u001b[0;36mLatentODEFunc.forward\u001b[0;34m(self, z, t)\u001b[0m\n\u001b[1;32m     18\u001b[0m t_expanded \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mexpand(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Concatenate z and t\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m z_t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_expanded\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compute derivative\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(z_t)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass using all data\n",
    "    x_reconstructed, mean, logvar = model(tensor_positions, all_times, tensor_time_deltas)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss, recon_loss, kl_loss = loss_function(\n",
    "        x_reconstructed.squeeze(1),  # Remove batch dimension\n",
    "        tensor_positions.unsqueeze(0).expand_as(x_reconstructed).squeeze(1),  # Match dimensions\n",
    "        mean, \n",
    "        logvar\n",
    "    )\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track loss\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        end_time = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.6f}, Recon Loss: {recon_loss.item():.6f}, KL Loss: {kl_loss.item():.6f}, Time: {end_time - start_time:.2f}s\")\n",
    "        start_time = end_time\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(loss)\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create uniform time grid for better visualization\n",
    "uniform_times_tensor = uniform_times.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reconstructed trajectory with uniform sampling\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get the initial latent state\n",
    "    z0, _, _ = model.encode(tensor_positions, tensor_time_deltas)\n",
    "    \n",
    "    # Solve ODE to get latent trajectory at uniform time steps\n",
    "    z_trajectory = model.ode_solver(z0.unsqueeze(0), uniform_times_tensor, return_whole_sequence=True)\n",
    "    \n",
    "    # Decode latent trajectory to observations\n",
    "    uniform_reconstructed = model.decoder(z_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy for plotting\n",
    "original_positions = positions\n",
    "uniform_times_np = uniform_times.cpu().numpy()\n",
    "uniform_reconstructed_np = uniform_reconstructed.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply inverse scaling to get back to original scale\n",
    "uniform_reconstructed_np = scaler.inverse_transform(uniform_reconstructed_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates for clarity\n",
    "orig_x, orig_y, orig_z = original_positions[:, 0], original_positions[:, 1], original_positions[:, 2]\n",
    "recon_x, recon_y, recon_z = uniform_reconstructed_np[:, 0], uniform_reconstructed_np[:, 1], uniform_reconstructed_np[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot original data points\n",
    "ax.scatter(orig_x, orig_y, orig_z, c='blue', marker='o', s=10, label='Original Data')\n",
    "\n",
    "# Plot reconstructed trajectory\n",
    "ax.plot(recon_x, recon_y, recon_z, c='red', linewidth=2, label='Reconstructed Trajectory')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('X Position')\n",
    "ax.set_ylabel('Y Position')\n",
    "ax.set_zlabel('Z Position')\n",
    "ax.set_title('UAV Trajectory: Original vs Latent ODE Reconstruction')\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('uav_trajectory_reconstruction.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bcc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual plots for each dimension vs time\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "# Original timestamps\n",
    "orig_times = timestamps - timestamps[0]\n",
    "\n",
    "# X position\n",
    "axs[0].scatter(orig_times, orig_x, c='blue', s=10, label='Original')\n",
    "axs[0].plot(uniform_times_np, recon_x, c='red', linewidth=2, label='Reconstructed')\n",
    "axs[0].set_ylabel('X Position')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('X Position over Time')\n",
    "\n",
    "# Y position\n",
    "axs[1].scatter(orig_times, orig_y, c='blue', s=10, label='Original')\n",
    "axs[1].plot(uniform_times_np, recon_y, c='red', linewidth=2, label='Reconstructed')\n",
    "axs[1].set_ylabel('Y Position')\n",
    "axs[1].legend()\n",
    "axs[1].set_title('Y Position over Time')\n",
    "\n",
    "# Z position\n",
    "axs[2].scatter(orig_times, orig_z, c='blue', s=10, label='Original')\n",
    "axs[2].plot(uniform_times_np, recon_z, c='red', linewidth=2, label='Reconstructed')\n",
    "axs[2].set_xlabel('Time (s)')\n",
    "axs[2].set_ylabel('Z Position')\n",
    "axs[2].legend()\n",
    "axs[2].set_title('Z Position over Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('uav_dimensions_over_time.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
